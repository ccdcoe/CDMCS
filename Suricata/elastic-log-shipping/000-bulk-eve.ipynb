{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requried modules with `pip`.\n",
    "\n",
    "```\n",
    "python3 -m pip install --user --upgrade elasticsearch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Elasticsearch(hosts=[\"localhost:9200\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object creation does not verify that server is up. Validate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic does not have schema, but each field still needs to be mapped to concrete type. If no mapping exist, then it will autodetect. While it mostly does a good job, you sometimes still want to make sure that mappings are correct. Furthermore, elastic achieves fast full text search by splitting large text segments into *tokens*. However, this can cause all kinds of problems when building a dashboard on top of tokenized strings. Solution is to dual map, so there's also a non-tokenized version.\n",
    "\n",
    "Not going too much into detail, many frontend tools rely on having a duplicate field for each string with `.keyword` suffix. If missing, many things may break. This is handled by defining this in a template.\n",
    "\n",
    "Normally, that template would be managed by Logstash or Filebeat. This is fine and good, but understanding this concept is very important down the line. **At one point you will be searching for something, only to discover that your field does not have right mapping to achieve whatever you want to do.** Then you need to modify the template and re-index if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {\n",
    "    \"index\": {\n",
    "        \"number_of_shards\": 3,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"refresh_interval\": \"5s\"\n",
    "    }\n",
    "}\n",
    "\n",
    "DEFAULT_PROPERTIES = {\n",
    "    \"@timestamp\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"strict_date_optional_time||epoch_millis||date_time\"\n",
    "    },\n",
    "    \"@version\": {\n",
    "        \"type\": \"keyword\"\n",
    "    },\n",
    "    \"ip\": {\n",
    "        \"type\": \"ip\"\n",
    "    }\n",
    "}\n",
    "\n",
    "DEFAULT_MAPPINGS = {\n",
    "    \"dynamic_templates\": [\n",
    "        {\n",
    "            \"message_field\": {\n",
    "                \"path_match\": \"message\",\n",
    "                \"mapping\": {\n",
    "                    \"norms\": False,\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"match_mapping_type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"string_fields\": {\n",
    "                \"mapping\": {\n",
    "                    \"norms\": False,\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"match_mapping_type\": \"string\",\n",
    "                \"match\": \"*\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"properties\": DEFAULT_PROPERTIES\n",
    "}\n",
    "\n",
    "DEFAULT_PATTERNS = [\n",
    "    \"logstash-*\",\n",
    "    \"events-*\",\n",
    "    \"suricata-*\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = conn.indices.put_template(\"default\", body={\n",
    "    \"order\": 0,\n",
    "    \"version\": 0,\n",
    "    \"index_patterns\": DEFAULT_PATTERNS,\n",
    "    \"settings\": DEFAULT_SETTINGS,\n",
    "    \"mappings\": DEFAULT_MAPPINGS,\n",
    "    \"aliases\": {}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load EVE JSON file, parse it and build the bulk. **Modify file path to reflect your EVE location.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODIFY_ME=\"/tmp/suricata/eve.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk flush done errors: False\n",
      "bulk flush done errors: False\n",
      "bulk flush done errors: False\n",
      "bulk flush done errors: False\n",
      "bulk flush done errors: False\n",
      "final bulk flush done errors: False\n"
     ]
    }
   ],
   "source": [
    "# init empty bulk\n",
    "BULK = []\n",
    "# how many items to send at once\n",
    "BULKSIZE = 10\n",
    "# counter to ensure that we flush when bulk is full\n",
    "COUNT = 0\n",
    "# Open EVE\n",
    "with open(PATH_MODIFY_ME, \"r\") as eve:\n",
    "    for line in eve:\n",
    "        # Parse EVE JSON\n",
    "        data = json.loads(line)\n",
    "        # add logstash-formatted timestamp key, needed by some apps\n",
    "        data[\"@timestamp\"] = data[\"timestamp\"]\n",
    "        # add more metadata to each event\n",
    "        data[\"path\"] = PATH_MODIFY_ME\n",
    "        # append metadata, this tells elastic which index to send the message to\n",
    "        BULK.append({\n",
    "            \"index\": {\n",
    "                # modify if you want to use different, note hardcoded date for simple example\n",
    "                \"_index\": \"suricata-2021.01.21\"\n",
    "            }\n",
    "        })\n",
    "        # add EVE message to bulk\n",
    "        BULK.append(data)\n",
    "        # check if bulk should be flushed\n",
    "        if COUNT%BULKSIZE==0:\n",
    "            # ship it!\n",
    "            resp = conn.bulk(BULK)\n",
    "            # give feedback, many tools are really bad at this step\n",
    "            print(\"bulk flush done errors:\", resp[\"errors\"])\n",
    "            # empty the bulk\n",
    "            BULK = []\n",
    "        # increment counter to decide if we should flush\n",
    "        COUNT += 1\n",
    "# flush the tail\n",
    "resp = conn.bulk(BULK)\n",
    "print(\"final bulk flush done errors:\", resp[\"errors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do `curl $IP:9200/suricata-*/_search` to verify that you have logs in elastic.\n",
    "\n",
    "If you have many EVE folders, then `glob` module is your friend in finding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/tmp/testcases/*/eve.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/testcases/00/eve.json', '/tmp/testcases/01/eve.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
